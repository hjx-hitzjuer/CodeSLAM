# === Default Config File ===
#
# Any possible parameter must be given here with description and a good default.
#


### DATA ###
DATA:
  # Name of the dataset. Some options will apply only for some datasets. Possible "replica"
  NAME: "replica"
  ROOT_DIR: "/media/hjx/2D97AD940A9AD661/SceneNet/train_0"
  # IMG_WIDTH must be divisible by 32
  IMG_WIDTH: 640
  IMG_HEIGHT: 480
  DTYPE: "float32"
  # If normalize the depth to [0, 1]
  NORMALIZE: False
  AVERAGE_DEPTH: 3.7584526086847587


### Augmentation to use. Only used for training ###
#AUG:
#  # If ANY is False, no augmentation will be used at all
#  # Use None for any augmentation to disable it
#  ANY: False
#
#  # Will use the same for all views of one tuple, by using kornia's same_on_batch=True
#  SAME_ON_VIEWS: False
#
#  # brightness, contrast, saturation, hue
#  COLOR_JITTER: (0.05, 0.05, 0.05, 0.05)
#
#  # kernel_size, angle (degrees), direction
#  MOTION_BLUR: (11, 70., 0.5)


### MODEL ###
MODEL:
  IN_CHANNEL: 3
  LATENT_DIM: 32


### LOSS ###
LOSS:
  # Possibilities are l1_depth (recommended), depth, abs_rel, and grad
  TERMS: ("l1_depth", )
  # Recommended weights are: l1_depth: 1.0, abs_rel: 1.0, depth: 1.0, grad: 0.01
  TERM_WEIGHTS: (1.0, )
  # Weights for the loss at each stage.
  STAGE_WEIGHTS: (1.0, 1.0, 1.0)


### TRAIN ###
TRAIN:
  EPOCHS: 50
  BATCH_SIZE: 32
  LR: 0.0001
  # In the last epoch, the learning rate will be LR*LR_SCHEDULE_FINAL_FRACTION
  LR_SCHEDULE_FINAL_FRACTION: 0.01
  # When using ddp the effective batch size is batch_size * num_nodes * num_gpus_per_node
  # to achieve similar training to the serial case, the learning rate should be scaled
  # accordingly: lr_ddp = batch_size_ddp/batch_size * lr = num_nodes * num_gpus_per_node * lr
  LR_DDP_SCALE_WITH_BATCH_SIZE: True
  # Options cpu, cuda (for one GPU), slurm-ddp (has to  use sbatch for launching), debug-ddp-num_nodes-num_gpus_per_node
  # For debug-ddp-1-1 one has to launch with MASTER_ADDR=localhost MASTER_PORT=12345
  # WORLD_SIZE=1 NODE_RANK=0 LOCAL_RANK=0 python train.py ...
  # See: https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-data-parallel
  DEVICE: "cuda"
  NUM_WORKERS: 6
  SHUFFLE: True
  DROP_LAST: True
  SEED: 1234
  # torch.backends.cudnn.benchmark set to True for good performance.
  # Might have to set false on 48G machine: https://github.com/pytorch/pytorch/issues/45769
  CUDNN_BENCHMARK: True


### IO ###
IO:
  LEVEL: "INFO"
  LOG_INTERVAL: 50
  # Weights summary at beginning of training ("full", "top", None)
  WEIGHTS_SUMMARY: None
  # Possible summaries to show in tensorboard: possibilities ("image", "depth", "confidence")
  SUMMARIES: ("image", "depth")
  # The number of samples per step used for scaling the x-axis of tensorboard correctly. This is set inside train.py.
  SAMPLES_PER_STEP: -1
